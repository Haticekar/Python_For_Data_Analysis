{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ac7f83",
   "metadata": {},
   "source": [
    "# Pandas Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd13c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haticekar/Desktop/anaconda/anaconda3/envs/python_course/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd \n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset('lukebarousse/data_jobs')\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Data Cleanup\n",
    "df['job_posted_date'] = pd.to_datetime(df['job_posted_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976ea46",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* `to_csv()`: Export DataFrame to CSV file. \n",
    "* `to_excel()`: Export DataFrame to Excel file.\n",
    "* `to_sql()`: Export DataFrame to SQL database.\n",
    "* `to_parquet()`: Export DataFrame to a parquet file. \n",
    "    * Parquet is a columnar storage file format that is designed for efficient data storage and retrieval.\n",
    "\n",
    "\n",
    "## General\n",
    "First, let's export our file to a **CSV file**. Often you may have to export files back and forth from dataframes to CSV files. Especially if you're trying to clean the data before using it in another data visualization tool like Tableau or PowerBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1644ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a CSV file\n",
    "df.to_csv(\"job_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfc7c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_csv in module pandas.core.generic:\n",
      "\n",
      "to_csv(path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, *, sep: 'str' = ',', na_rep: 'str' = '', float_format: 'str | Callable | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | list[str]' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, mode: 'str' = 'w', encoding: 'str | None' = None, compression: 'CompressionOptions' = 'infer', quoting: 'int | None' = None, quotechar: 'str' = '\"', lineterminator: 'str | None' = None, chunksize: 'int | None' = None, date_format: 'str | None' = None, doublequote: 'bool_t' = True, escapechar: 'str | None' = None, decimal: 'str' = '.', errors: 'OpenFileErrors' = 'strict', storage_options: 'StorageOptions | None' = None) -> 'str | None' method of pandas.core.frame.DataFrame instance\n",
      "    Write object to a comma-separated values (csv) file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path_or_buf : str, path object, file-like object, or None, default None\n",
      "        String, path object (implementing os.PathLike[str]), or file-like\n",
      "        object implementing a write() function. If None, the result is\n",
      "        returned as a string. If a non-binary file object is passed, it should\n",
      "        be opened with `newline=''`, disabling universal newlines. If a binary\n",
      "        file object is passed, `mode` might need to contain a `'b'`.\n",
      "    sep : str, default ','\n",
      "        String of length 1. Field delimiter for the output file.\n",
      "    na_rep : str, default ''\n",
      "        Missing data representation.\n",
      "    float_format : str, Callable, default None\n",
      "        Format string for floating point numbers. If a Callable is given, it takes\n",
      "        precedence over other numeric formatting parameters, like decimal.\n",
      "    columns : sequence, optional\n",
      "        Columns to write.\n",
      "    header : bool or list of str, default True\n",
      "        Write out the column names. If a list of strings is given it is\n",
      "        assumed to be aliases for the column names.\n",
      "    index : bool, default True\n",
      "        Write row names (index).\n",
      "    index_label : str or sequence, or False, default None\n",
      "        Column label for index column(s) if desired. If None is given, and\n",
      "        `header` and `index` are True, then the index names are used. A\n",
      "        sequence should be given if the object uses MultiIndex. If\n",
      "        False do not print fields for index names. Use index_label=False\n",
      "        for easier importing in R.\n",
      "    mode : {'w', 'x', 'a'}, default 'w'\n",
      "        Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n",
      "        the file opening. Typical values include:\n",
      "    \n",
      "        - 'w', truncate the file first.\n",
      "        - 'x', exclusive creation, failing if the file already exists.\n",
      "        - 'a', append to the end of file if it exists.\n",
      "    \n",
      "    encoding : str, optional\n",
      "        A string representing the encoding to use in the output file,\n",
      "        defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "        is a non-binary file object.\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        Set to ``None`` for no compression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for faster compression and to create\n",
      "        a reproducible gzip archive:\n",
      "        ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "    \n",
      "           May be a dict with key 'method' as compression mode\n",
      "           and other entries as additional compression options if\n",
      "           compression mode is 'zip'.\n",
      "    \n",
      "           Passing compression options as keys in dict is\n",
      "           supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
      "    quoting : optional constant from csv module\n",
      "        Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "        then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "        will treat them as non-numeric.\n",
      "    quotechar : str, default '\\\"'\n",
      "        String of length 1. Character used to quote fields.\n",
      "    lineterminator : str, optional\n",
      "        The newline character or character sequence to use in the output\n",
      "        file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "        this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "    \n",
      "        .. versionchanged:: 1.5.0\n",
      "    \n",
      "            Previously was line_terminator, changed for consistency with\n",
      "            read_csv and the standard library 'csv' module.\n",
      "    \n",
      "    chunksize : int or None\n",
      "        Rows to write at a time.\n",
      "    date_format : str, default None\n",
      "        Format string for datetime objects.\n",
      "    doublequote : bool, default True\n",
      "        Control quoting of `quotechar` inside a field.\n",
      "    escapechar : str, default None\n",
      "        String of length 1. Character used to escape `sep` and `quotechar`\n",
      "        when appropriate.\n",
      "    decimal : str, default '.'\n",
      "        Character recognized as decimal separator. E.g. use ',' for\n",
      "        European data.\n",
      "    errors : str, default 'strict'\n",
      "        Specifies how encoding and decoding errors are to be handled.\n",
      "        See the errors argument for :func:`open` for a full list\n",
      "        of options.\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None or str\n",
      "        If path_or_buf is None, returns the resulting csv format as a\n",
      "        string. Otherwise returns None.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    read_csv : Load a CSV file into a DataFrame.\n",
      "    to_excel : Write DataFrame to an Excel file.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create 'out.csv' containing 'df' without indices\n",
      "    \n",
      "    >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "    ...                    'mask': ['red', 'purple'],\n",
      "    ...                    'weapon': ['sai', 'bo staff']})\n",
      "    >>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n",
      "    \n",
      "    Create 'out.zip' containing 'out.csv'\n",
      "    \n",
      "    >>> df.to_csv(index=False)\n",
      "    'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      "    >>> compression_opts = dict(method='zip',\n",
      "    ...                         archive_name='out.csv')  # doctest: +SKIP\n",
      "    >>> df.to_csv('out.zip', index=False,\n",
      "    ...           compression=compression_opts)  # doctest: +SKIP\n",
      "    \n",
      "    To write a csv file to a new folder or nested folder you will first\n",
      "    need to create it using either Pathlib or os:\n",
      "    \n",
      "    >>> from pathlib import Path  # doctest: +SKIP\n",
      "    >>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "    >>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
      "    >>> df.to_csv(filepath)  # doctest: +SKIP\n",
      "    \n",
      "    >>> import os  # doctest: +SKIP\n",
      "    >>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
      "    >>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f1d63",
   "metadata": {},
   "source": [
    "Another popular form to export to for your non code savy friends is to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5aeb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to an Excel file\n",
    "import openpyxl\n",
    "df.to_excel(\"jobs_data.xlsx\", index=False)\n",
    "#conda install openpyxl indirilmesi lazım python environment ında "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdaa02",
   "metadata": {},
   "source": [
    "Then, is writing to a **SQL database**. This will only work if you actually have a database to write to. In our case we don't, so this is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae75837f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785741"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the DataFrame to a SQL database\n",
    "\n",
    "# this requires a connection to a SQL database, we'll use sqlalchemy for this\n",
    "# !conda install -c anaconda sqlalchemy -y\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///jobs.db')\n",
    "\n",
    "df.to_sql('job_table', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863630b",
   "metadata": {},
   "source": [
    "You are also able to export a Dataframe to a **parquet file** or **pickle file**. We won't be going into that during the video, but it's good to be aware of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a Parquet file\n",
    "df.to_parquet('jobs_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a Pickle file\n",
    "df.to_pickle('job_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e27fcc",
   "metadata": {},
   "source": [
    "## Export our DataFrame \n",
    "Let's go back to the DataFrame we created in the last section from our actual job postings DataFrame and export it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6f0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title_short</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_via</th>\n",
       "      <th>job_schedule_type</th>\n",
       "      <th>job_work_from_home</th>\n",
       "      <th>search_location</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_no_degree_mention</th>\n",
       "      <th>job_health_insurance</th>\n",
       "      <th>job_country</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_year_avg</th>\n",
       "      <th>salary_hour_avg</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_type_skills</th>\n",
       "      <th>job_posted_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Guadalajara, Jalisco, Mexico</td>\n",
       "      <td>via BeBee México</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>2023-01-14 13:18:07</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hewlett Packard Enterprise</td>\n",
       "      <td>['r', 'python', 'sql', 'nosql', 'power bi', 't...</td>\n",
       "      <td>{'analyst_tools': ['power bi', 'tableau'], 'pr...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst (m/f/d)</td>\n",
       "      <td>Nuremberg, Germany</td>\n",
       "      <td>via Big Country Jobs</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-01-19 14:05:05</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Symanto</td>\n",
       "      <td>['python', 'r', 'sql', 'azure', 'power bi', 'e...</td>\n",
       "      <td>{'analyst_tools': ['power bi', 'excel', 'power...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Tampa, FL</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Florida, United States</td>\n",
       "      <td>2023-01-19 13:19:45</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citi</td>\n",
       "      <td>['sql', 'python', 'unix', 'excel', 'jira']</td>\n",
       "      <td>{'analyst_tools': ['excel'], 'async': ['jira']...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Projects &amp; Solutions Data Analyst (UK Pensions)</td>\n",
       "      <td>Birmingham, UK</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2023-01-04 13:35:45</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aon</td>\n",
       "      <td>['sql', 'excel']</td>\n",
       "      <td>{'analyst_tools': ['excel'], 'programming': ['...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Base work from home job/internship at Mga...</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>True</td>\n",
       "      <td>India</td>\n",
       "      <td>2023-01-14 13:11:58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mgadz Inc</td>\n",
       "      <td>['sas', 'sas', 'sql']</td>\n",
       "      <td>{'analyst_tools': ['sas'], 'programming': ['sa...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56381</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Oracle Supply Chain Data Analyst</td>\n",
       "      <td>Quezon City, Metro Manila, Philippines</td>\n",
       "      <td>via Trabajo.org</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>2023-03-11 06:29:07</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>emerson</td>\n",
       "      <td>['oracle']</td>\n",
       "      <td>{'cloud': ['oracle']}</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56382</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>Jena, Jerman</td>\n",
       "      <td>melalui XING</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-03-12 06:18:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linimed Gruppe</td>\n",
       "      <td>['sql', 'julia', 'power bi', 'dax']</td>\n",
       "      <td>{'analyst_tools': ['power bi', 'dax'], 'progra...</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56383</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amul Careers 2023 - Apply Online - Data Analys...</td>\n",
       "      <td>India</td>\n",
       "      <td>melalui Jobsleworld - Jobs In India - Job Vaca...</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>2023-03-13 06:16:28</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amul</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56384</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data &amp; Analytics Architect (w/m/x)</td>\n",
       "      <td>Erfurt, Jerman</td>\n",
       "      <td>melalui LinkedIn</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-03-12 06:18:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NTT DATA DACH</td>\n",
       "      <td>['aws', 'azure']</td>\n",
       "      <td>{'cloud': ['aws', 'azure']}</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56385</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>CRM Data Analyst</td>\n",
       "      <td>Bad Rodach, Jerman</td>\n",
       "      <td>melalui BeBee Deutschland</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-03-12 06:18:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HABA FAMILYGROUP</td>\n",
       "      <td>['sas', 'sas', 'sql', 'excel']</td>\n",
       "      <td>{'analyst_tools': ['sas', 'excel'], 'programmi...</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56386 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_title_short  ... job_posted_month\n",
       "0        Data Analyst  ...              Jan\n",
       "1        Data Analyst  ...              Jan\n",
       "2        Data Analyst  ...              Jan\n",
       "3        Data Analyst  ...              Jan\n",
       "4        Data Analyst  ...              Jan\n",
       "...               ...  ...              ...\n",
       "56381    Data Analyst  ...              Mar\n",
       "56382    Data Analyst  ...              Mar\n",
       "56383    Data Analyst  ...              Mar\n",
       "56384    Data Analyst  ...              Mar\n",
       "56385    Data Analyst  ...              Mar\n",
       "\n",
       "[56386 rows x 18 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DA = df[(df['job_title_short'] == 'Data Analyst')].copy()\n",
    "df_DA['job_posted_month'] = df_DA['job_posted_date'].dt.strftime('%b')\n",
    "\n",
    "months = df_DA['job_posted_month'].unique()\n",
    "df_DA_month = {}\n",
    "for month in months:\n",
    "    df_DA_month[month] = df_DA[df_DA['job_posted_month'] == month].copy()\n",
    "\n",
    "quarter_1 = [df_DA_month['Jan'], df_DA_month['Feb'], df_DA_month['Mar']]\n",
    "df_concat = pd.concat(quarter_1, ignore_index=True)\n",
    "\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec5b4c",
   "metadata": {},
   "source": [
    "Now we can save our jobs to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0071772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a CSV file\n",
    "df_concat.to_csv('jobs_1st_quarter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ab2d3",
   "metadata": {},
   "source": [
    "DataFrame.to_csv(path_or_buf=None, *, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', lineterminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)\n",
    "* Write object to a comma-separated values (csv) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71d314",
   "metadata": {},
   "source": [
    "### 📦 Exporting Data Nedir?\n",
    "\n",
    "* Exporting data (veri dışa aktarma), bir program ya da analiz ortamı içinde oluşturulan veya işlenen verinin harici bir dosyaya kaydedilmesi işlemidir.\n",
    "\n",
    "* Pandas kullanıyorsan, bu genelde şu anlama gelir:\n",
    "✅ DataFrame'ini bir dosya olarak dışa aktarmak\n",
    "(örneğin: .csv, .xlsx, .json, .sql, .parquet, .pkl gibi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0898dac",
   "metadata": {},
   "source": [
    "### 🔍 Neden Exporting (Dışa Aktarma) Yapılır?\n",
    "1. 🔄 Veriyi başka bir sistemde kullanmak için\n",
    "(örneğin: veri tabanına yüklemek, başka bir programda açmak)\n",
    "2. 🧑‍💻 İş arkadaşlarıyla paylaşmak için\n",
    "(örneğin: Excel dosyası paylaşmak)\n",
    "3. 🧠 Analiz sonuçlarını saklamak için\n",
    "(örneğin: model çıktılarını .csv olarak kaydetmek)\n",
    "4. 🗃 Veri yedekleme ve arşivleme için\n",
    "(örneğin: .pkl veya .parquet gibi formatlarda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c814f",
   "metadata": {},
   "source": [
    "### 🛠 Pandas ile Yaygın Export Formatları:\n",
    "\n",
    "| Format      | Fonksiyon           | Açıklama|\n",
    "|-------------|---------------------|---------------------------------------|\n",
    "|.csv        | df.to_csv()         | En yaygın düz metin formatı|\n",
    "|.xlsx       | df.to_excel()       | Excel dosyasına aktarma|\n",
    "|.json       | df.to_json()        | JSON formatına aktarma|\n",
    "|.sql        | df.to_sql()         | SQL veri tabanına yazma|\n",
    "|.parquet    | df.to_parquet()     | Hızlı ve sıkıştırılmış veri formatı|\n",
    "|.pkl        | df.to_pickle()      | Python nesnesi olarak saklama|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4f74b",
   "metadata": {},
   "source": [
    "* df.to_csv(\"veri.csv\", index=False)\n",
    "* df.to_excel(\"veri.xlsx\", index=False)\n",
    "* df.to_sql(\"tablo\", con=engine)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
